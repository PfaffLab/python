#!/usr/bin/python
#
# make-mappability-graph.py
# 
# shawn driscoll
# 9 dec 2014
#
# transform input fasta sequences into kmers and map to 
# a reference with bowtie. for each query sequence the 
# mappability index is calculated versus any target 
# sequences any of the query's mers map to. the index is 
# M/N where M is the number of shared mers and N is the 
# total mers generated by that query sequence.
# when aligning a fasta to itself redundant pairs are 
# replaced with a single pair with the best mappability
# score. 
#

import os,sys,re
import argparse
from hashlib import md5
import subprocess as sp
from warnings import warn

def main(args):

	# variables
	refs = {}
	graph_final = {}
	rname = ""
	rseq = ""
	fifo_sam = "bwt_fifo.sam"
	acurrent = ""
	aln_history = {}
	qcurrent = ""
	qlast = ""
	refs_hit = {}
	bins = {}
	use_bins = False
	bins_hit = {}

	reads_tot = 0
	reads_aligned = 0
	reads_unique = 0
	reads_ambig = 0
	reads_multi = 0

	# if -B then parse it in
	if len(args.B) > 0:
		fin = open(args.B, "r")
		for szl in fin:
			aln = szl.strip().split("\t")
			bins[aln[0]] = aln[1]

		fin.close()

		use_bins = True

	# fork and run the mer generation/alignment

	try:
		os.unlink(fifo_sam)
	except:
		# don't do anything
		pass

	sys.stderr.write("+mkfifo {}\n".format(fifo_sam))
	os.mkfifo(fifo_sam)
	pid = os.fork()
	if pid==0:
		# child process 
		sys.stderr.write("forked!\n")
		make_and_align_mers(args, fifo_sam)
		os._exit(0)

	# continue on in parent...

	# open fifo and parse alignments
	fin = open(fifo_sam, "r")
	for szl in fin:
		if re.search("^\@", szl):
			# in header
			if re.search("^\@SQ", szl):
				# reference name
				aln = szl.strip().split("\t")
				rid = aln[1].split(":")
				rlen = aln[2].split(":")
				# create entry for this reference with length of sequence
				# and slots for unique hit count and total hit count (and total binned hit count
				# if use_bins is True)
				if use_bins:
					refs[rid[1]] = [rlen[1], 0, 0, 0]
				else:
					refs[rid[1]] = [rlen[1], 0, 0]

		else:
			# not in the header, treat as an alignment

			aln = szl.strip().split("\t")
			# get read name out
			tmp = aln[0].split("__")
			qcurrent = tmp[0]

			if qcurrent != qlast:
				reads_tot += 1

				# finished parseing out mer alignments for last read name
				if len(refs_hit.keys()) > 0:
					# handle the hits
					reads_aligned += 1

					# pass hits to a list
					lhits = []
					for rid in refs_hit.keys():
						lhits.append(list([rid, refs_hit[rid]]))

					if len(lhits) > 1:
						# sort the list by the total hits
						lhits.sort(key=lambda x:x[1])
						lhits.reverse()

						if args.best:
							# keep only the highest # hits
							hbest = lhits[0][1]
							i = 1
							while i < len(lhits):
								if lhits[i][1] < hbest:
									break
								i += 1

							lhits = lhits[0:i]


					if len(lhits)==1:
						reads_unique += 1

						# increment unique hit count and total hit count
						refs[lhits[0][0]][1] += 1
						refs[lhits[0][0]][2] += 1
						refs[lhits[0][0]][3] += 1

					else:
						# read hit multiple features (and if best then equally well)

						if use_bins:
							# sort out bins
							for i in range(len(lhits)):
								bins_hit[bins[lhits[i][0]]] = 0

							bfactor = 1
							if len(bins_hit.keys()) > 1:
								bfactor = 1.0/len(bins_hit.keys())**2

							# if bfactor is < 1 then read hit multiple bins

						reads_ambig += 1
						hfactor = 1.0/len(lhits)

						for i in range(len(lhits)):
							refs[lhits[i][0]][2] += hfactor

						if use_bins:
							if bfactor < 1:
								reads_multi += 1

							for i in range(len(lhits)):
								refs[lhits[i][0]][3] += hfactor*bfactor

							bins_hit = {}

				refs_hit = {}

			if not (int(aln[1]) & 0x4):
				# aligned so deal with it
				if aln[2] not in refs_hit:
					refs_hit[aln[2]] = 0

				refs_hit[aln[2]] += 1


			qlast = qcurrent
			

	fin.close()
	os.unlink(fifo_sam)

	# deal with the last bucket
	if len(refs_hit.keys()) > 0:
		# handle the hits
		reads_aligned += 1

		# pass hits to a list
		lhits = []
		for rid in refs_hit.keys():
			lhits.append(list([rid, refs_hit[rid]]))

		if len(lhits) > 1:
			# sort the list by the total hits
			lhits.sort(key=lambda x:x[1])
			lhits.reverse()

			if args.best:
				# keep only the highest # hits
				hbest = lhits[0][1]
				i = 1
				while i < len(lhits):
					if lhits[i][1] < hbest:
						break
					i += 1

				lhits = lhits[0:i]


		if len(lhits)==1:
			reads_unique += 1

			# increment unique hit count and total hit count
			refs[lhits[0][0]][1] += 1
			refs[lhits[0][0]][2] += 1

		else:

			reads_ambig += 1
			hfactor = 1.0/len(lhits)

			for i in range(len(lhits)):
				refs[lhits[i][0]][2] += hfactor



	# print stats
	sys.stderr.write("""
processed {} total reads; of these:
{} ({:0.2f}%) were aligned:
  unique:       {} ({:0.2f}%)
  ambiguous:    {} ({:0.2f}%)
  multi-gene:   {} ({:0.2f}%)

""".format(reads_tot, 
		reads_aligned, float(reads_aligned)/reads_tot*100,  
		reads_unique, float(reads_unique)/reads_aligned*100,
		reads_ambig, float(reads_ambig)/reads_aligned*100,
		reads_multi, float(reads_multi)/reads_aligned*100))


	for kid in sorted(refs.keys()):
		lout = [kid] + refs[kid]
		print "\t".join(map(str, lout))

	return 0

# --
# parses query fasta, makes mers and writes them to a fifo which is 
# read by bowtie in a subprocess. alignments are written to a second
# fifo which is read by the parent thread.
def make_and_align_mers(args, fifo_name):

	# vars
	mers = []
	nmers = 0
	seq = ""
	mk = ""
	qname = ""
	lnum = 0
	bowtie_cmd = "bowtie -p {} -v {} -afS {} mer_fifo.fa".format(args.p, args.v, args.index)

	# make fifo for writing mers

	try:
		os.unlink("mer_fifo.fa")
	except:
		pass

	os.mkfifo("mer_fifo.fa")
	sys.stderr.write("+mkfifo {}\n".format("mer_fifo.fa"))

	fifo_out = open(fifo_name, "w")
	sys.stderr.write("opened sam fifo\n")
	sys.stderr.write("CMD: {}\n".format(bowtie_cmd))
	p1 = sp.Popen(bowtie_cmd.split(), stdout=fifo_out)
	sys.stderr.write("started bowtie in subprocess\n")

	fin = open(args.fq, "r")
	fout = open("mer_fifo.fa", "w")
	sys.stderr.write("opened mer fifo for writing\n")

	for szl in fin:
		szl = szl.strip()
		lnum += 1

		if lnum==1:
			qname = szl.split(" ")[0]
			qname = re.sub("^\@", "", qname)
		elif lnum==2:
			# this is the sequence
			seq = szl
			mers = make_mers(seq, args)
			nmers = len(mers)
			nidx = 0
			for mk in mers:
				fout.write(">{}__{}__{}\n{}\n".format(qname, nmers, nidx, mk))
				nidx += 1

		if lnum==4:
			lnum = 0			


	fin.close()

	fout.close()
	p1.wait()
	fifo_out.close()
	os.unlink("mer_fifo.fa")

	return(0)


# --
# generate unique kmers from sequence
def make_mers(seq, opts):
	num_mers = len(seq) - opts.k + 1
	mers = {}

	i = 0
	while i < num_mers:
		mer = seq[i:(i+opts.k)]
		mers[mer] = 0
		i += opts.s

	return(mers.keys())


def file_exists(fname):
	try:
		fin = open(fname)
	except IOError:
		return False

	fin.close()
	return True

#==============================================================================
# main entry point
#==============================================================================

parser = argparse.ArgumentParser(
	description="""Quantify reads against reference by kmers""")

parser.add_argument("fq", type=str, 
	help="FASTQ reads")
parser.add_argument("index", type=str, 
	help="bowtie[1] aligner index for the FASTA file")

# quantification options
parser.add_argument("-k", type=int, default=25, action="store", 
	help="mer length [25]")
parser.add_argument("-s", type=int, default=1, action="store",
	help="mer-to-mer step size [1]")
parser.add_argument("-p", type=int, default=1, action="store",
	help="number of threads for bowtie [1]")
parser.add_argument("-v", type=int, default=0, action="store",
	help="allowed mismatches when mapping kmers to reference transcripts (the -v option of bowtie) [0]")

parser.add_argument("-B", dest="B", action="store", type=str, default="",
	help="Transcript bundle file. A 2-column file with transcript ids and bundle ids. May be the result of a sequence clustering of transcripts into loci or genes.")
parser.add_argument("--best", dest="best", action="store_const", const=True, default=False, 
	help="Enable best mode which only assigns hits to the best match (the one with the most kmers). If there are multiple bests then the hit is divided between the targets.")


args = parser.parse_args()

if __name__ == "__main__":

	try:
		sys.exit(main(args))
	except KeyboardInterrupt:
		sys.stderr.write("\nkilled it\n")


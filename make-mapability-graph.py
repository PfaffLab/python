#!/usr/bin/python
#
# make-mappability-graph.py
# 
# shawn driscoll
# 9 dec 2014
#
# transform input fasta sequences into kmers and map to 
# a reference with bowtie. for each query sequence the 
# mappability index is calculated versus any target 
# sequences any of the query's mers map to. the index is 
# M/N where M is the number of shared mers and N is the 
# total mers generated by that query sequence.
# when aligning a fasta to itself redundant pairs are 
# replaced with a single pair with the best mappability
# score. 
#

import os,sys
import argparse
from hashlib import md5
import subprocess as sp
from warnings import warn

def main(args):

	# variables
	refs = {}
	graph_final = {}
	rname = ""
	rseq = ""
	fifo_sam = "bwt_fifo.sam"
	acurrent = ""
	aln_history = {}
	qcurrent = ""
	qlast = ""

	# fork and run the mer generation/alignment

	try:
		os.unlink(fifo_sam)
	except:
		# don't do anything
		pass

	sys.stderr.write("+mkfifo {}\n".format(fifo_sam))
	os.mkfifo(fifo_sam)
	pid = os.fork()
	if pid==0:
		# child process 
		sys.stderr.write("forked!\n")
		make_and_align_mers(args, fifo_sam)
		os._exit(0)

	# continue on in parent...

	# open fifo and parse alignments
	fin = open(fifo_sam, "r")
	for szl in fin:
		aln = szl.strip().split("\t")

		if int(aln[1]) & 0x4:
			continue

		tmp = aln[0].split("|")

		# track query name (without the mer infomation) to clear out the 
		# history hash since at least the queries are coming back in order
		qcurrent = tmp[0]
		if qcurrent != qlast:
			aln_history = {}

		qlast = qcurrent

		# concatenate the query mer name and the target name so that we can track
		# hits and avoid double-counts
		acurrent = aln[0] + "|" + aln[2]

		# don't double count mers that map to the same reference more than 1 time
		if acurrent in aln_history:
			#sys.stderr.write("note: skipping duplicate alignment\n")
			continue
		else:
			aln_history[acurrent] = 0

		kid = "{}|{}".format(tmp[0], aln[2])
		if kid not in refs:
			# add in the combination and the query's total mer count
			refs[kid] = [tmp[1], 0]

		# add this hit
		refs[kid][1] += 1

	fin.close()
	os.unlink(fifo_sam)

	sys.stderr.write("consolidating redundant pairs...\n")
	for kid in refs.keys():
		ratio = float(refs[kid][1])/float(refs[kid][0])
#		if ratio > 1:
#			ratio = 1.0

		tmp = kid.split("|")
		kid0 = "{}|{}".format(tmp[1], tmp[0])

		if (kid in graph_final):
			# one of the two is in there
			if graph_final[kid] < ratio:
				graph_final[kid] = ratio
		elif (kid0 in graph_final):
			if graph_final[kid0] < ratio:
				graph_final[kid0] = ratio
		else:
			graph_final[kid] = ratio

	for kid in graph_final:
		tmp = kid.split("|")
		print "\t".join([tmp[0], tmp[1], "{:0.4f}".format(graph_final[kid])])

	return 0

# --
# parses query fasta, makes mers and writes them to a fifo which is 
# read by bowtie in a subprocess. alignments are written to a second
# fifo which is read by the parent thread.
def make_and_align_mers(args, fifo_name):

	# vars
	mers = []
	nmers = 0
	seq = ""
	mk = ""
	rname = ""
	bowtie_cmd = "bowtie -p {} -v {} --sam-nohead -afS {} mer_fifo.fa".format(args.p, args.v, args.index)

	# make fifo for writing mers

	try:
		os.unlink("mer_fifo.fa")
	except:
		pass

	os.mkfifo("mer_fifo.fa")
	sys.stderr.write("+mkfifo {}\n".format("mer_fifo.fa"))

	fifo_out = open(fifo_name, "w")
	sys.stderr.write("opened sam fifo\n")
	sys.stderr.write("CMD: {}\n".format(bowtie_cmd))
	p1 = sp.Popen(bowtie_cmd.split(), stdout=fifo_out)
	sys.stderr.write("started bowtie in subprocess\n")

	fin = open(args.fa, "r")
	fout = open("mer_fifo.fa", "w")
	sys.stderr.write("opened mer fifo for writing\n")

	for szl in fin:
		szl = szl.strip()
		if szl[0] == ">":
			# new ref, dump mers to bowtie process
			if len(seq) > 0:
				mers = make_mers(seq, args)

				nmers = len(mers)
				nidx = 0
				for mk in mers:
					fout.write(">{}|{}|{}\n{}\n".format(rname, nmers, nidx, mk))
					nidx += 1

			# setup new ref
			tmp = szl.split(" ")
			rname = tmp[0][1:]
			seq = ""
		else:
			seq = seq + szl

	fin.close()

	if len(seq) > 0:
		mers = make_mers(seq, args)
		nmers = len(mers)
		nidx = 0
		for mk in mers:
			fout.write(">{}|{}|{}\n{}\n".format(rname, nmers, nidx, mk))
			nidx += 1

	fout.close()
	p1.wait()
	fifo_out.close()
	os.unlink("mer_fifo.fa")

	return(0)


# --
# generate unique kmers from sequence
def make_mers(seq, opts):
	num_mers = len(seq) - opts.k + 1
	mers = {}

	i = 0
	while i < num_mers:
		mer = seq[i:(i+opts.k)]
		mers[mer] = 0
		i += opts.s

	return(mers.keys())


def file_exists(fname):
	try:
		fin = open(fname)
	except IOError:
		return False

	fin.close()
	return True

#==============================================================================
# main entry point
#==============================================================================

parser = argparse.ArgumentParser(
	description="""Cluster sequences in input FASTA""")

parser.add_argument("fa", type=str, 
	help="FASTA reference")
parser.add_argument("index", type=str, 
	help="bowtie[1] aligner index for the FASTA file")

# quantification options
parser.add_argument("-k", type=int, default=25, action="store", 
	help="mer length [25]")
parser.add_argument("-s", type=int, default=1, action="store",
	help="mer-to-mer step size [1]")
parser.add_argument("-p", type=int, default=1, action="store",
	help="number of threads for bowtie [1]")
parser.add_argument("-v", type=int, default=0, action="store",
	help="allowed mismatches when mapping kmers to reference transcripts (the -v option of bowtie) [0]")

args = parser.parse_args()

if __name__ == "__main__":

	try:
		sys.exit(main(args))
	except KeyboardInterrupt:
		sys.stderr.write("\nkilled it\n")

